# -*- coding: utf-8 -*-
"""President_Botrick_Purgatory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H27SucrTh4EzwcOMyXODVf0DqjvmtYtt
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True) #connecting to Google Drive

!pip install transformers==4.16.2 #installing the transformers library, which allows access to our models
from transformers import BertTokenizerFast #importing libraries for deep learning functions and models
from transformers import AutoTokenizer, AutoModelWithLMHead
import pandas as pd
import transformers
import torch
from torch.nn import Dropout
from keras.preprocessing.sequence import pad_sequences
from torch.optim import Adam
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import get_linear_schedule_with_warmup
from transformers import GPT2TokenizerFast, AdamW
import numpy as np
from torch import nn
from torch.nn import CrossEntropyLoss
import random
from transformers import BertModel

#defining model and its architecture, which consists of BERT layer, followed by a linear layer for classification
class Model(torch.nn.Module):
    def __init__(self,
                 model_name_or_path: str,
                 dropout: float,
                 num_labels: int):
        super(Model, self).__init__()
        self.bert_model = BertModel.from_pretrained(model_name_or_path)
        self.dropout = Dropout(dropout)
        self.num_labels = num_labels
        self.classifier = nn.Linear(self.bert_model.config.hidden_size, num_labels)

    def forward(self,
                input_ids: torch.tensor,
                attention_mask: torch.tensor,
                token_type_ids: torch.tensor,
                label: torch.tensor = None
                ):
        output = self.bert_model(input_ids=input_ids,
                                                            attention_mask=attention_mask,
                                                            token_type_ids=token_type_ids)
        logits = self.classifier(self.dropout(output.pooler_output))

        loss_fct = CrossEntropyLoss()
        # Compute losses if labels provided
        if label is not None:
            loss = loss_fct(logits.view(-1, self.num_labels), label.type(torch.long))
        else:
            loss = torch.tensor(0)

        return logits, loss

maxlen = 200
device = torch.device("cuda")
bertmodel = Model('bert-base-uncased', 0.1, 2)
bertmodel.to(device)
bertmodel.load_state_dict(torch.load('/content/drive/MyDrive/President_Botrick/model.pt'))
berttokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True, model_max_length=maxlen) #intialize tokenizer

gpttokenizer = GPT2TokenizerFast.from_pretrained('gpt2-medium') 
gptmodel = torch.load('/content/drive/MyDrive/President_Botrick/model01250.pt')

outputfile = open('good_lines.txt', 'w')
contexts = ['White bears', '']

def generate():
  for i in range(100):

    input_context = x
    input_ids = gpttokenizer.encode(input_context, return_tensors='pt')
    outputs = gptmodel.generate(input_ids=input_ids, max_length=maxlen, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)
    decoded = gpttokenizer.decode(outputs[0])
    decoded = decoded.replace("<|endoftext|>","")
    decoded = decoded.replace('{APPLAUSE}', '')
    print(decoded)
    outputfile.write(decoded + '\n')
    return(decoded)

def classify(decoded):
  tokens = [berttokenizer.encode(decoded)]
  tokens = [token[0:maxlen] for token in tokens]
  test_input_ids = pad_sequences(tokens, maxlen=maxlen, dtype="long", value=0.0, truncating="post", padding="post")
  print(test_input_ids.shape)
  test_masks = [[float(i != 0.0) for i in ii] for ii in test_input_ids]
  test_data = TensorDataset(torch.tensor(test_input_ids), torch.tensor(test_masks))
  test_sampler = SequentialSampler(test_data)
  test_data_loader = DataLoader(test_data, sampler = test_sampler, batch_size = 1)
  with torch.no_grad():
    for batch in test_data_loader:
      batch = tuple(t.to(device) for t in batch)
      i, m = batch
      logits, _ = bertmodel(i, m, None, None)
      predictions = torch.argmax(logits, dim=1)
      logits = logits.cpu().numpy()
      props = 1 / (1 + np.exp(-1 * logits))
      print(str(props[0][1] * 100) + '% to be applauded')
  return(predictions)

num = 100
count = 0


for i in range(num):
  decoded = generate()
  predictions = classify(decoded).cpu().numpy()[0]
  if(predictions == 1):
    count += 1
  print(str(count) + '|' + str(count/(i+1)))